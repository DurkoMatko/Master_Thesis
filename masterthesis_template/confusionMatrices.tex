\paragraph{Terminology} Before diving into talk about the metrics, there are 4 crucial terms which need to be explained:
\begin{itemize}
\item \textbf{Tue Positives (TP)} - instances correctly labeled as positive
\item \textbf{True Negatives (TN)}- instances correctly labeled as negative
\item \textbf{False Positives (FP)} - instances incorrectly labeled as positive
\item \textbf{False Negatives (FN)}- instances incorrectly labeled as negative
\end{itemize}


The metrics that you choose to evaluate your machine learning algorithms are very important and not all are suitable for every situation. Choice of metrics influences how the performance of machine learning algorithms is measured and choosing a wrong evaluation metric for particular use could potentially lead towards eliminating the best performing algorithm in favour of the worse one.
Here are some of the most often used metrics used to evaluate classification algorithms. It's also useful to choose the metric before doing the analysis, so you won't get distracted by already having the results in case of doing the decision later.

\begin{itemize}
\item \textbf{Classification accuracy} - this is the most intuitive and common evaluation metric for classification problems but it is also the most misused one. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case. In case of imbalanced dataset with 9\% of instances in one class and only 10\% in the other, predicting every instance as a majority class without even considering its features would lead to high accuracy of 90\%. This is called \textbf{accuracy paradox.}
\[ Accuracy = \frac{TP + TN}{TP + TN + FP + FN}\]
\item \textbf{Confusion matrix} - clean and unambiguous way to present the prediction results of a classifier. If the classification is binary (there are only 2 classes), this matrix has 2 rows and 2 columns - therefore altogether 4 cells which are filled with true/false positives/negatives count. Such scenario is demonstrated in table \ref{table:Confusion_matrix_general}. Although the confusion matrix shows all of the information about the classifier's performance, more meaningful measures can be extracted from it to illustrate certain performance criteria.\cite{bradley1997use}. 
\begin{table}[H]
{
\centering
\begin{tabular}{ |p{4cm}|p{4cm}|p{4cm}|  }
 \hline
 \multicolumn{3}{|c|}{Confusion matrix} \\
 \hline
  & Predicted positive & Predicted negative\\
 \hline
 Real positive   & TP    &FN\\ \hline
 Real negative &   FP  & TN\\ \hline
\end{tabular}
}
\caption{Confusion matrix}
\label{table:Confusion_matrix_general}
\end{table}

\item \textbf{Precision} - Precision can be seen as a representation of a classifiers exactness. A low precision can also indicate a large number of False Positives. If the precision is high, it says that there's a high probability of positive label being True Positive. It cannot be tricked but it also hides a lot.
\[ Precision = \frac{TP}{TP + FP}\]
\item \textbf{Recall} - also called \textit{sensitivity} or the \textit{True Positive Rate}, it is a number of True Positives divided by the number of True Positives and the number of False Negatives. In other words, it is a ratio of how many of all positive instances have been identified. Recall can be tricked (labeling all as majority class) but if used next to precision, it gives extra information
\[ Recall = \frac{TP}{TP + FN}\]
\item \textbf{F1 score} - as already mentioned, precision hides some facts and recall can be tricked. To give the full story, they need to be used together. That's what F1 measure (F measure) is for. It is the harmonic average of the precision and recall, where its best value is at 1.
\[ F = 2 * \frac{precision * recall}{precision + recall}\]
\end{itemize}Nice example to demonstrate difference between precision and recall is the concept of Indian Jurisprudence, where "100 culprits may let go free but no innocent should be punished". If we let go so many culprits in order to ensure no innocent is punished, recall will be pretty low, but precision very high.