
In the thesis, I have tried to answer following questions:
\begin{itemize}
\item{\textbf{$RQ_{1}$}: Do the OSS projects which release more often get general better sentiment score on social media?}
\item{\textbf{$RQ_{2}$}: Does a release have an immediate effect on sentiment?}
\item{\textbf{$RQ_{3}$}: Is there a correlation between sentiment change and size of the release (number of commits) ?}
\item{\textbf{$RQ_{4}$}: Is it possible to link social media entries to their respective bugs which they talk about?}
\end{itemize}

I have examined, analysed and researched every question and then offered my results to field experts. I asked 200 open source developers to provide insights on my results and whether these align with their personal opinions and experience.

\textbf{$RQ_{1}$}: Anwser on this question was provided in Subsection  \ref{ssec:weekly}. Results obtained in this section are clearly in favour of this hypothesis. There is a significant sentiment difference between all 3 sentiment groups I have worked with. Hypothesis also found a some support among OSS experts as two thirds of them answered the question they would expect higher sentiment for more releasing frameworks. On the other side, one third is big enough not to be ignored. Some answers questioned the release frequencies of buckets and some pointed out that it just comes to quality of code and not the release frequency:

\begin{itemize}
\item \textit{"I don't necessarily agree that this is a split yes or no answer. A project could release constantly, but if they're constantly shipping buggy code, it'll have a negative sentiment overall."}
\item \textit{"Releasing less then once per month doesn't seem negative to me. Think you'd need a bigger range like once every 1-3 months, once per 6-12 months etc."}
\item \textit{"More often releases generally means that lots of patches for bugs are coming out, which leads to happier devs and a happier community."}
\end{itemize}

\textbf{$RQ_{2}$}:What I have hoped for while formulating this question was an immediate sentiment change among users in the very first days after a release. This case was examined and partially validated by the results presented in Subsection \ref{ssec:beforeAfter}. As always, it depends on the way how results are interpreted. One of 3 release frequency groups registered a significant sentiment change of sentiment between tweets prior and past a release. On the other side, only 2 of 11 projects did (both were the part of seldom releasing group). This obviously leads to question whether the chosen projects were just the overall outliers or the seldom releasing projects do really experience disappointment after releases (as the users expect more from them, when they do not happen as often). Also field experts were not able to agree on the expected outcome. 55\% said  it is a surprise that sentiment got worse while 45\% said it does not surprise them at all. Interesting point here was the comment about releasing fixes which actually destroy the workarounds around bugs:
\begin{itemize}
\item \textit{"Projects that don't update frequently are giving more time for buggy behavior to cement into the community and have code baked around it. This leads to breakage when the bugs are patched."}
\item \textit{"Generally speaking I think it's difficult for most projects to keep every developer happy, and typically after a release people are reminded about the project and therefore will be more vocal. I would expect sentiment and volume of tweets to reduce as time passes from the release date."}
\end{itemize}

\textbf{$RQ_{3}$}:This question was studied in subsections number \ref{ssec:crossCorrelation} and \ref{ssec:crossCorrelationCommits}. The results have shown that there is definitely a difference between counting releases and counting commits numbers within releases. The real question is if there is any pattern in the commits/sentiment relationship. Sadly, I have not found any strong relationship here, as the projects ended up in all 4 quadrants - commits positively/negatively effecting sentiment, sentiment positively/negatively effecting commit count in the next months. 80\% experts agreed that the size and type of the release play a role in user sentiment while the rest did not notice any sentiment changes:
\begin{itemize}
\item \textit{"I generally see higher sentiment on minor releases (every 6 weeks, include new features) than with patch releases (irregular, frequent, only fix bugs) because people are excited for new features."}
\item \textit{"A high number of commits around a release could indicate that there are regressions (especially recently with ember-data :( ). But generally, I'm surprised that more commits for ember decreases sentiment. "}
\end{itemize}

\textbf{$RQ_{4}$}: This task proved to be very difficult and I did not achieve the results I hoped for in regards of linking particular matches together. Whole work is described in Chapter \ref{chap:pairingBugs}. But while working on this task I found some interesting relationships in the data. During the analysis, I have noticed that the similarity calculated for the whole Stack Overflow or Reddit discussion is higher than just a similarity of the particular comment referring the bug. Response in the survey approved of this as more than 77\% would expect this. Respondents agreed that in the whole discussion, the knowledge of a group is shared and the likelihood of particular comment diverging is higher than the whole "averaged" discussion. Experts also provided several interesting answers:
\begin{itemize}
\item \textit{Overall discussion of a problem will lead to group knowledge being shared, whereas the initial comment is just the knowledge of one person. There is a higher likelihood that any individual comment will diverge from any other comment about a particular topic, but the average evens out because all information is shared over time."}
\item \textit{"The shorter the SO discussion, the more likely it is clearly an issue recorded on git(hub - I assume)... maybe?"}
\item \textit{"Seems like longer descriptions are more likely to be similar, maybe because they are copy pasted? I would check for a bias in the underlying similarity finder though
"}
\item \textit{"Seems like there's a pretty high correlation between similarity of short length GitHub issue discussions and longer SO discussions. Could be that bugs are reported on SO first, then discussed, then re-reported and opened on GitHub once the underlying cause is found."}
\end{itemize} I have created a plots for similarities of Stack Questions with 20 random Git issues for the projects. Among these histograms I mixed one which showed just similarities of real Git-Stack matches talking about the very same issue. More than 55\% of people have chosen the right histogram when asked to choose the one which represents matches. Compared to statistical average of 25\%, this is a huge increase so there definitely is a textual similarity between Git-Stack matches.


Also some extra general questions were in my survey and got very nice response regarding accuracy of my analysis. 55\% of respondents said that community sentiment in their particular projects is/was accurate to my findings. 45\% responses stated they have not been contributing to the project long enough to say. Noone expressed direct disagreement with my results.