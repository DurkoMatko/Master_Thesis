
In the thesis, I've tried to answer following questions:
\begin{itemize}
\item{\textbf{$RQ_{1}$}: Do the OSS projects which release more often get general better sentiment score on social media?}
\item{\textbf{$RQ_{2}$}: Does a release have an immediate effect on sentiment?}
\item{\textbf{$RQ_{3}$}: Is there a correlation between sentiment change and size of the release (number of commits) ?}
\item{\textbf{$RQ_{4}$}: Is it possible to successfully pair social media entries to their respective bugs which they talk about?}
\end{itemize}

I have examined, analyzed and researched every question but especially with data science it is always good to get an outside look from the field experts. That's why I have decided to ask 200 open source developers to provide insights on my results and whether they align with their personal opinions and experience.

\textbf{$RQ_{1}$}: Anwser on this question was provided in subsection  \ref{ssec:weekly}. Results obtained in this section are clearly in favor of this hypothesis. There is a significant sentiment difference between all 3 sentiment groups I have worked with. Hypothesis also found a some support among OSS experts as two thirds of them answered the question they would expect higher sentiment for more releasing frameworks. On the other side, one third is big enough not to be neglected. Some answers questioned the release frequencies of buckets and some pointed out that it just comes to quality of code and not the release frequency.

\textbf{$RQ_{2}$}:What I have hoped for while formulating this question was an immediate sentiment change among users in the very first days after a release. This case was examined and partially validated by the results presented in subsection \ref{ssec:beforeAfter}. As always, it depends on the way how results are interpreted. One of 3 release frequency groups registered a significant sentiment change of sentiment between tweets prior and past a release. On the other side, only 2 of 11 projects did (both were the part of seldom releasing group). This obviously leads to question whether the chosen projects were just the overall outliers or the seldom releasing projects do really experience disappointment after releases (as the users expect more from them, when they don't happen as often). Also field experts weren't able to agree on the expected outcome. 55\% said  it is a surprise that sentiment got worse while 45\% said it doesn't surprise them at all. Interesting point here was that \textit{"projects that don't update frequently are giving more time for buggy behavior to cement into the community and have code baked around it. This leads to breakage when the bugs are patched."}

\textbf{$RQ_{3}$}:This question was studied in subsections \ref{ssec:crossCorrelation} and \ref{ssec:crossCorrelationCommits}. The results have shown that there is definitely a difference between counting releases and counting commits numbers within releases. The real question is if there is any pattern in the commits/sentiment relationship. Sadly, I haven't found any strong relationship here, as the projects ended up in all 4 quadrants - commits positively/negatively effecting sentiment, sentiment positively/negatively effecting commit count in the next months. 

\textbf{$RQ_{4}$}: This task proved to be very difficult and I didn't achieve any results in regards of pairing particular matches together. Whole work is described in chapter \ref{chap:pairingBugs} But while working on this task I found some interesting relationships in the data. During the analysis, I have noticed that the similarity calculated for the whole Stack Overflow or Reddit discussion is higher than just a similarity of the particular comment referring the bug. Response in the survey approved of this as more than 77\% would expect this. Respondents agreed that in the whole discussion, the knowledge of a group is shared and the likelihood of particular comment diverging is higher than the whole "averaged" discussion. I've also created a plots for similarities of Stack Questions with 20 random Git issues for the projects. Among these histograms I've mixed one which showed just similarities of real Git-Stack matches talking about the very same issue. More than 55\% of people have chosen the right histogram when asked to choose the one which represents matches. Compared to statistical average of 25\%, this is a huge increase so there definitely is a textual similarity between Git-Stack matches.


I've also asked some extra general questions in my survey and got very nice response regarding accuracy of my analysis. 55\% of respondents said that community sentiment in their particular projects is/was accurate to my findings. 45\% responses stated they have not been contributing to the project long enough to say.