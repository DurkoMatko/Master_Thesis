Thesis itself combines many fields and areas. Its main topics are Git mining, Twitter mining, sentiment analysis and text similarity.\\
\\
Nice example to Git mining is work done by Russel and Mathew \cite{russell2013mining}. They discovered 9 interesting perils and according to their paper, most projects are inactive and have low count of commits. Two thirds of repositories are personal and large portion of these are not software development projects. Also, pull-requests are not very popular feature and many projects do not even conduct all their software to the Git. Tsay et al. \cite{tsay2012social} developed two measures of project success - Developer Attention and Work Contribution. They found out that projects with highly socially connected developers are not necessarily the most active or popular projects and projects with a high level of developer multitasking surprisingly tend to receive less Developer Attention, but greater Work Contribution. Little evidence that usage of specific social media features in GitHub directly correlates with software project success was found.  Jarczyk et al. \cite{jarczyk2014github} created in their work 2 own metrics - one of them being Quality of Support. They surprisingly shown that higher number of developers has positive effect only on fixing bugs in the long term, not on the quick response within 3 days. Takhteyev et al. \cite{takhteyev2010investigating} explored developers' geographic locations by examining self-reported information available within GitHub profiles. They confirmed the hypothesis that largest share of users are registered from United States - 39 \%. A research has also been done regarding availability and retrieval improvement of Git data (\cite{wagstrom2013network} and \cite{gousios2012ghtorrent}). Sentiment analysis of Git commits was described in Guzman et al \cite{guzman2014sentiment}. They found that language used in the project has an affect on sentiment. Java projects showed the worse sentiment. Also, commit messages on Monday tend to show worse emotions. Christley and Scott \cite{christley2007analysis} extracted 29 open-source related activity types
based on social positions and activity patters. Instead of GitHub data, they used Sourceforge.net
dataset. Sadly, they didnâ€™t continue in their work to analyse the effect of the quantitative population for these positions within various projects.\\
\\
Last mentioned paper brings me to another part of my thesis, which was sentiment analysis of social media, mostly Twitter data. A continuous series of Twitter evaluation exercise called SemEval organized by SIGLEX show how big and complex task Twitter sentiment analysis is as several teams continue working on couple of shared tasks. This project historically started already in 1998 and after 4 conferences once every 3 years, it takes place yearly from 2012. As stated in Rosenthal et al. \cite{rosenthal2017semeval}) summary, in 2017 was the count of participating teams 48. The most common technologies used across all the teams were Python (numpy and sklearn libraries), Java, Weka or NLTK and the most common external lexicon was was Sentiment140. This justifies my selection of training data in Section \ref{sec:trainingDatasets}, as well as used language and the particular library in Section \ref{sec:languageProcessingTools}. Like I mention in Section \ref{ssec:GettingData}, Twitter is a default choice for social media sentiment analysis. That means lot of work has already been done in this area. Agarwal et al. \cite{agarwal2011sentiment} acquired 11,875 tweets from commercial source, translated them to English using Google Translate and after deleting bad translations and stratified sampling to get balanced dataset, their dataset's size was 5127 tweets. After preprocessing, they calculate 50 various features for each document (tweet). They used unigram as their baseline model and offered a comprehensive set of experiments using Tree kernel and Senti-features models. Compared to the baseline, they reported 4\% gain for both binary and 3-way classification. Koloumpis et al. \cite{kouloumpis2011twitter} used various feature types (n-gram, lexicon, POS and microblogging) and showed that using POS tagger might not be useful for sentiment analysis of Twitter and therefore microblogging sphere in general. Pak and Paroubek \cite{pak2010twitter} worked with considerably bigger dataset of 300,000 tweets evenly distributed among 3 main sentiments. Their approach is using Twitter API and exact same steps as Go et al. \cite{go2009twitter} and Read in 2015, but because of its latest changes it might not be usable anymore. My workaround for this issue is described in Subsection \ref{ssec:GettingData}. They queried for positive and negative emoticons to build an arbitrarily large training dataset which is one of their contributions. This is an interesting approach as emoticons do carry lot of emotions but in case of irony can be misleading. They trained 2 classifiers based on Naive Bayes and experimented with different features as bigrams, trigrams or unigrams and also part-of-speech (POS) distribution information. The best performance was achieved with bigrams and a very high accuracy with a low decision value was obtained. As shown in the previous references, the consensus usually is that preprocessing should get rid of as much noise as possible. But Saif et al. \cite{saif2012semantic} are on the other side of the spectrum. They demonstrated that not removing stopwords can actually increase accuracy. Apart from this, they have also defined new so called "semantic features" which are assigned to each entity within a tweet. In their paper, they shown that using these features increased a performance of two-class Naive Bayes classifier. While textual features are analysed to the biggest detail, emoticons and emojis are mostly overlooked or preprocessed (also my case in Section \ref{sec:classifierEvaluation}, Table \ref{table:preprocesingAccuracy}). Hogenboom et al.\cite{hogenboom2013exploiting} created sentiment framework which does text-based and emoticon-based analysis separately and they merge the results later. Novak et al. \cite{novak2015sentiment} went even step further and decided to analyse the new generation of emoticons - emojis. They drew a sentiment map of the 751 emojis, compare the differences between the tweets with and without emojis, the differences between the more and less frequent emojis and their positions in tweets.\\
\\
Much less saturated is the topic of linking social media bug entries and  git repositories. I have not found any paper tackling this problem. Bachman et al. \cite{bachmann2010missing} pointed out that not all known bugs are reported through one channel only (e.g GitHub's issue system) what is important to keep in mind during my work described in Chapter \ref{chap:pairingBugs}. The closest topic to this is probably duplicate bug detection as it also requires extracting and understanding of bug descriptions.  Bug analysis most similar to my work was done by Runeson et al. \cite{runeson2007detection}. They implement and test a technique of detecting bug duplicates using similar techniques to mine in Section \ref{sec:pairingApproaches}. Bug reports are turned into stream of stemmed tokens and stopwords are removed. On top of these procedures their work shares with mine, they replace synonyms. For similarity calculation, they used 3 measures - cosine, dice and Jaccard but eventually decided to continue only with cosine as their main choice. I'm also using cosine what makes our work even more similar. Results of this study showed that about 2/3 of the duplicates can possibly be found. Similar task was tackled by Jalbert and Weimer \cite{jalbert2008automated} who introduced an algorithm which flags duplicate bug reports on their arrival. They claim to reduce the development costs by 8\% while still allowing at least one reported issue per bug to reach developers. They pointed out that inverse document frequency was not useful for their task. Algorithm by Anvik et al. \cite{anvik2005coping} assigns incoming bug reports to correct developers based on the textual features of the report with 57\% precision for Eclipse project. Using bugs reported for Firefox, unique bugs are identified 90\% of time and their duplicates correctly assigned 28\% of times. This shows that my results presented in Section \ref{sec:similarityResults} are reasonable. Weiss et al. \cite{weiss2007long} predict time required to fix a bug based on the description similarity with bugs from the past. For similarity calculation, they used Lucene engine developed by Apache and worked only with bug title and description. I've chosen the same approach in Subsubsection \ref{ssec:issuesMining}.
\\
\\
Thesis has 2 separate parts, each tackling its own unique task. Both parts utilize simple Term Frequency Inverse Document Frequency (TF-IDF) to extract word relevance from text. It has been discussed and used for this task in many papers over the years. Lan et al. \cite{lan2005comprehensive} came up with their own comparative scheme called \textit{tf.rf} where $IDF=log(N/n_i)$ was replaced by $rf=log(1-n_i/n_i-)$. Using this scheme with linear SVM on McNemar's significance tests \cite{dietterich1998approximate} shown better performance than 4 other TF-IDF variants. As described in Section \ref{sec:classifierEvaluation}, I'm also using and testing SVM with TF-IDF. The tests were executed on Newsgroup and Reuters corpus.